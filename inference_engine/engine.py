# inference_engine/engine.py
# ========================================================================
# flask_image_app/inference_engine/engine.py
# åŒ»ç–—æŠ¥å‘Šç”Ÿæˆå¼•æ“ - å¢å¼ºç‰ˆï¼ˆæ·»åŠ å¤šæ ·æ€§é‡‡æ ·å’Œè°ƒè¯•åŠŸèƒ½ï¼‰
# ========================================================================

import os
import torch
import pickle
from PIL import Image
from torchvision import transforms

# âœ… å¯¼å…¥å¢å¼ºç‰ˆçš„æ¨¡å‹å®šä¹‰
from .model_definition import IUReportGenerator


class MedicalReportEngine:
    """åŒ»ç–—æŠ¥å‘Šç”Ÿæˆå¼•æ“ï¼ˆå¢å¼ºç‰ˆï¼‰"""

    def __init__(self, config_dict, debug=False):
        """
        åˆå§‹åŒ–å¼•æ“ã€‚
        :param config_dict: ä¸€ä¸ªåŒ…å«æ‰€æœ‰å¿…è¦é…ç½®é¡¹çš„å­—å…¸ã€‚
        :param debug: æ˜¯å¦å¯ç”¨è°ƒè¯•æ¨¡å¼
        """
        self.config = config_dict
        self.debug = debug
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        if self.debug:
            print(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        # --- âœ… æ™ºèƒ½å¤„ç† IMG_SIZE é…ç½® ---
        img_size = self.config['IMG_SIZE']
        if isinstance(img_size, int):
            resize_args = (img_size, img_size)
        elif isinstance(img_size, (list, tuple)) and len(img_size) == 2:
            resize_args = tuple(img_size)
        else:
            raise ValueError(f"IMG_SIZE must be an int or a tuple/list of length 2. Got: {img_size}")

        self.transform = transforms.Compose([
            transforms.Resize(resize_args),
            transforms.ToTensor(),
            transforms.Normalize(mean=self.config['IMG_MEAN'], std=self.config['IMG_STD'])
        ])
        self.model = None
        self.vocab = None
        self._load_model_and_vocab()

    def _load_model_and_vocab(self):
        """åŠ è½½æ¨¡å‹æƒé‡å’Œè¯æ±‡è¡¨"""
        model_path = self.config['MODEL_PATH']
        vocab_path = self.config['VOCAB_PATH']

        if not os.path.exists(model_path) or not os.path.exists(vocab_path):
            self.model = None
            self.vocab = None
            print("âŒ è­¦å‘Š: æ¨¡å‹æˆ–è¯æ±‡è¡¨æ–‡ä»¶æœªæ‰¾åˆ°ï¼ŒAIæŠ¥å‘ŠåŠŸèƒ½å°†ä¸å¯ç”¨ã€‚")
            return

        # åŠ è½½è¯æ±‡è¡¨
        with open(vocab_path, 'rb') as f:
            self.vocab = pickle.load(f)
        
        if self.debug:
            print(f"ğŸ“š è¯æ±‡è¡¨å¤§å°: {len(self.vocab.get('idx2word', {}))}")

        # ä½¿ç”¨ä¼ å…¥çš„é…ç½®å­—å…¸æ¥å®ä¾‹åŒ–æ¨¡å‹
        self.model = IUReportGenerator(
            vocab_size=self.config['VOCAB_SIZE'],
            cnn_out_features=self.config['CNN_OUT_FEATURES'],
            lstm_hidden_size=self.config['LSTM_HIDDEN_SIZE'],
            lstm_num_layers=self.config['LSTM_NUM_LAYERS'],
            lstm_dropout=self.config['LSTM_DROPOUT'],
        )

        checkpoint = torch.load(model_path, map_location=self.device, weights_only=True)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.to(self.device).eval()
        print("âœ… åŒ»ç–—æŠ¥å‘Šå¼•æ“åŠ è½½æˆåŠŸï¼")

    def generate(
        self, 
        image_path: str,
        temperature: float = 0.8,
        top_k: int = 50,
        top_p: float = 0.9,
        use_sampling: bool = True
    ) -> str:
        """
        è¾“å…¥å›¾åƒè·¯å¾„ï¼Œè¿”å›ç”Ÿæˆçš„æŠ¥å‘Šæ–‡æœ¬
        
        å‚æ•°è¯´æ˜ï¼š
        - temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.7-1.0æ¨èï¼‰ï¼Œè¶Šé«˜è¶Šéšæœº
        - top_k: Top-Ké‡‡æ ·ï¼ˆ30-50æ¨èï¼‰ï¼Œé™åˆ¶å€™é€‰è¯æ•°é‡
        - top_p: Top-Pé‡‡æ ·ï¼ˆ0.85-0.95æ¨èï¼‰ï¼ŒNucleusé‡‡æ ·
        - use_sampling: æ˜¯å¦ä½¿ç”¨é‡‡æ ·ï¼ˆTrueæ¨èï¼ŒFalseåˆ™ä½¿ç”¨è´ªå©ªè§£ç ï¼‰
        """
        if self.model is None or self.vocab is None:
            return "AIæŠ¥å‘ŠåŠŸèƒ½æš‚ä¸å¯ç”¨ã€‚"

        try:
            # åŠ è½½å¹¶é¢„å¤„ç†å›¾åƒ
            image = Image.open(image_path).convert('RGB')
            tensor = self.transform(image).unsqueeze(0).to(self.device)
            
            if self.debug:
                print(f"ğŸ–¼ï¸  å›¾åƒå½¢çŠ¶: {tensor.shape}")
                print(f"ğŸ² ç”Ÿæˆå‚æ•°: temperature={temperature}, top_k={top_k}, top_p={top_p}, sampling={use_sampling}")

            # ç”ŸæˆæŠ¥å‘ŠIDåºåˆ—
            with torch.no_grad():
                output_ids = self.model.generate_report(
                    tensor,
                    sos_id=self.config['SOS_TOKEN_ID'],
                    eos_id=self.config['EOS_TOKEN_ID'],
                    max_len=self.config['MAX_REPORT_LEN'],
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    use_sampling=use_sampling
                )
            
            if self.debug:
                print(f"ğŸ“ ç”Ÿæˆçš„token IDs (å‰20ä¸ª): {output_ids[0][:20].cpu().tolist()}")

            # å°†IDè½¬æ¢ä¸ºæ–‡æœ¬
            words = []
            token_ids = output_ids[0].cpu().numpy()
            
            for idx in token_ids:
                if idx == self.config['EOS_TOKEN_ID']:
                    break
                if idx not in [self.config['PAD_TOKEN_ID'], self.config['SOS_TOKEN_ID']]:
                    word = self.vocab['idx2word'].get(int(idx), '<unk>')
                    if word not in ['<UNK>', '<unk>', '<pad>', '<PAD>']:
                        words.append(word)
            
            if self.debug:
                print(f"ğŸ“„ ç”Ÿæˆçš„è¯è¯­: {words[:15]}...")

            # æ‹¼æ¥å¹¶æ ¼å¼åŒ–æŠ¥å‘Š
            report = " ".join(words).strip()
            
            # åŸºæœ¬çš„åå¤„ç†
            if report:
                # ç¡®ä¿å¥å­ä»¥å¥å·ç»“å°¾
                if not report.endswith('.'):
                    report += '.'
                # é¦–å­—æ¯å¤§å†™
                report = report[0].upper() + report[1:] if len(report) > 1 else report.upper()
            else:
                report = "No significant findings."
            
            if self.debug:
                print(f"âœ… æœ€ç»ˆæŠ¥å‘Š: {report}")
            
            return report

        except Exception as e:
            error_msg = f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}"
            if self.debug:
                import traceback
                print(f"âŒ {error_msg}")
                traceback.print_exc()
            return error_msg
    
    def generate_multiple(
        self, 
        image_path: str, 
        num_samples: int = 3,
        temperature: float = 0.8,
        top_k: int = 50,
        top_p: float = 0.9
    ) -> list:
        """
        ä¸ºåŒä¸€å¼ å›¾ç‰‡ç”Ÿæˆå¤šä¸ªä¸åŒçš„æŠ¥å‘Š
        
        è¿™å¯¹äºè¯Šæ–­å¤šæ ·æ€§é—®é¢˜å¾ˆæœ‰ç”¨
        """
        reports = []
        for i in range(num_samples):
            if self.debug:
                print(f"\n--- ç”ŸæˆæŠ¥å‘Š #{i+1} ---")
            report = self.generate(
                image_path,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                use_sampling=True
            )
            reports.append(report)
        return reports
