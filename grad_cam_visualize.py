# ========================================================================
# grad_cam_visualize.py - Grad-CAM çƒ­åŠ›å›¾å¯è§†åŒ–ï¼ˆä¿®å¤ç‰ˆï¼‰
# é€‚é…æ¨¡å‹: ç–¾ç—…æ„ŸçŸ¥çš„CNN + LSTMæŠ¥å‘Šç”Ÿæˆæ¨¡å‹
# ========================================================================
import os
import sys
import torch
import torch.nn.functional as F
from PIL import Image
import numpy as np
import cv2
import matplotlib.pyplot as plt

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import Config
from inference_engine.engine import MedicalReportEngine


class GradCAM:
    """Grad-CAM å®ç°ï¼ˆä¿®å¤ç‰ˆï¼‰"""

    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.feature_maps = None
        self.gradients = None

        # æ³¨å†Œ hookï¼ˆä¿®å¤ç­¾åï¼‰
        self.target_layer.register_forward_hook(self.save_feature_maps)
        self.target_layer.register_full_backward_hook(self.save_gradients)

    def save_feature_maps(self, module, input, output):
        """ä¿å­˜ç‰¹å¾å›¾ - æ­£ç¡®çš„hookç­¾å"""
        self.feature_maps = output.detach()

    def save_gradients(self, module, grad_input, grad_output):
        """ä¿å­˜æ¢¯åº¦ - æ­£ç¡®çš„hookç­¾å"""
        self.gradients = grad_output[0].detach()

    def __call__(self, image_tensor, max_len=10):
        """
        ç”Ÿæˆ Grad-CAM çƒ­åŠ›å›¾
        :param image_tensor: é¢„å¤„ç†åçš„å›¾åƒ [1, C, H, W]
        :param max_len: æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ§åˆ¶è®¡ç®—é‡ï¼‰
        :return: çƒ­åŠ›å›¾ numpy array [H, W]
        """
        # ä¿å­˜åŸå§‹æ¨¡å¼
        was_training = self.model.training
        self.model.train()
        image_tensor.requires_grad_(True)

        try:
            with torch.enable_grad():
                # å‰å‘ä¼ æ’­ï¼šè·å–ç‰¹å¾å›¾å’Œå…¨å±€ç‰¹å¾
                feature_map, global_features = self.model.encoder(image_tensor)  # [B, 512, 7, 7], [B, 512]

                # è·å–ç–¾ç—…ç‰¹å¾
                disease_logits = self.model.disease_classifier(global_features)
                disease_features = torch.sigmoid(disease_logits)  # [B, 14]

                # åˆå§‹åŒ–LSTMçŠ¶æ€
                B = image_tensor.size(0)
                h = torch.zeros(1, B, 512, device=image_tensor.device)
                c = torch.zeros(1, B, 512, device=image_tensor.device)

                # å‡†å¤‡ç‰¹å¾ç”¨äºæ³¨æ„åŠ›
                features_for_attn = feature_map.view(B, 512, -1).permute(0, 2, 1)  # [B, 49, 512]

                # ç¼–ç ç–¾ç—…ç‰¹å¾
                disease_context = self.model.decoder.disease_encoder(disease_features)  # [B, 256]

                # ç¬¬ä¸€ä¸ªtoken: SOS
                sos_id = 1  # å‡è®¾SOS token IDæ˜¯1
                input_ids = torch.tensor([[sos_id]], dtype=torch.long, device=image_tensor.device)

                # æ‰§è¡Œä¸€æ­¥è§£ç 
                embedding = self.model.decoder.embedding(input_ids[:, -1])  # [B, 512]
                
                # è®¡ç®—æ³¨æ„åŠ›
                context, _ = self.model.decoder.attention(
                    features_for_attn, h[-1], disease_features
                )  # [B, 512]
                
                # LSTMè¾“å…¥
                lstm_input = torch.cat(
                    (embedding, context, disease_context), dim=1
                ).unsqueeze(1)  # [B, 1, 1280]
                
                out, (h, c) = self.model.decoder.lstm(lstm_input, (h, c))
                logits = self.model.decoder.fc(out.squeeze(1))  # [B, vocab_size]

                # å¯¹æ‰€æœ‰logitæ±‚å’Œä½œä¸ºloss
                score = logits[0].sum()

                # åå‘ä¼ æ’­
                self.model.zero_grad()
                score.backward(retain_graph=False)

            # === è®¡ç®— Grad-CAM ===
            if self.gradients is None or self.feature_maps is None:
                raise RuntimeError("æœªèƒ½æ•è·æ¢¯åº¦æˆ–ç‰¹å¾å›¾")

            gradients = self.gradients  # [1, 512, 7, 7]
            feature_maps = self.feature_maps  # [1, 512, 7, 7]

            # å…¨å±€å¹³å‡æ± åŒ–æ¢¯åº¦ â†’ æƒé‡ [512]
            weights = torch.mean(gradients, dim=[0, 2, 3])  # [512]

            # åŠ æƒæ±‚å’Œç‰¹å¾å›¾
            cam = torch.zeros(feature_maps.shape[2:], device=feature_maps.device)  # [7, 7]
            for i, w in enumerate(weights):
                cam += w * feature_maps[0, i, :, :]

            cam = F.relu(cam)
            cam = cam.cpu().numpy()
            cam = cv2.resize(cam, (image_tensor.shape[3], image_tensor.shape[2]))  # [224, 224]
            cam = cam - np.min(cam)
            cam = cam / (np.max(cam) + 1e-8)

            return cam

        finally:
            # æ¢å¤æ¨¡å‹åŸå§‹æ¨¡å¼
            if was_training:
                self.model.train()
            else:
                self.model.eval()


def preprocess_image(image_path, img_size=(224, 224), mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):
    """å›¾åƒé¢„å¤„ç†"""
    from torchvision import transforms
    image = Image.open(image_path).convert('RGB')
    resized_image = image.resize(img_size, Image.Resampling.LANCZOS)
    original_for_overlay = np.array(resized_image)

    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=std)
    ])
    tensor = transform(resized_image).unsqueeze(0)
    return tensor, original_for_overlay


def overlay_heatmap(original_img, cam, alpha=0.6):
    """å°†çƒ­åŠ›å›¾å åŠ åˆ°åŸå›¾"""
    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)
    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)
    overlay = (alpha * heatmap + (1 - alpha) * original_img).astype(np.uint8)
    return overlay


def main():
    if len(sys.argv) < 2:
        print("ä½¿ç”¨æ–¹æ³•: python grad_cam_visualize.py <å›¾ç‰‡è·¯å¾„>")
        sys.exit(1)

    image_path = sys.argv[1]
    if not os.path.exists(image_path):
        print(f"é”™è¯¯: å›¾ç‰‡ä¸å­˜åœ¨ {image_path}")
        sys.exit(1)

    print(f"ğŸ–¼ï¸  åŠ è½½å›¾ç‰‡: {image_path}")

    # === åˆå§‹åŒ–å¼•æ“ ===
    config = Config()
    engine_config = {
        'MODEL_PATH': config.MODEL_PATH,
        'VOCAB_PATH': config.VOCAB_PATH,
        'IMG_SIZE': config.IMG_SIZE,
        'IMG_MEAN': config.IMG_MEAN,
        'IMG_STD': config.IMG_STD,
        'VOCAB_SIZE': config.VOCAB_SIZE,
        'CNN_OUT_FEATURES': config.CNN_OUT_FEATURES,
        'LSTM_HIDDEN_SIZE': config.LSTM_HIDDEN_SIZE,
        'LSTM_NUM_LAYERS': config.LSTM_NUM_LAYERS,
        'LSTM_DROPOUT': config.LSTM_DROPOUT,
        'MAX_REPORT_LEN': config.MAX_REPORT_LEN,
        'PAD_TOKEN_ID': config.PAD_TOKEN_ID,
        'SOS_TOKEN_ID': config.SOS_TOKEN_ID,
        'EOS_TOKEN_ID': config.EOS_TOKEN_ID,
    }

    engine = MedicalReportEngine(config_dict=engine_config, debug=True)
    model = engine.model

    if model is None:
        print("âŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        sys.exit(1)

    # === è·å– ResNet çš„ layer4 ===
    target_layer = model.encoder.features[7]  # resnet.layer4
    print(f"ğŸ¯ Hook ç›®æ ‡å±‚: model.encoder.features[7] (ResNet layer4)")

    # === é¢„å¤„ç†å›¾åƒ ===
    input_tensor, original_img = preprocess_image(
        image_path,
        img_size=config.IMG_SIZE,
        mean=config.IMG_MEAN,
        std=config.IMG_STD
    )
    input_tensor = input_tensor.to(engine.device)

    # === ç”Ÿæˆ Grad-CAM ===
    print("ğŸ”¥ ç”ŸæˆGrad-CAMçƒ­åŠ›å›¾...")
    grad_cam = GradCAM(model, target_layer)
    cam = grad_cam(input_tensor, max_len=10)

    # === å åŠ çƒ­åŠ›å›¾ ===
    overlay = overlay_heatmap(original_img, cam)

    # === ä¿å­˜ç»“æœ ===
    base_name = os.path.splitext(os.path.basename(image_path))[0]
    output_path = f"gradcam_{base_name}.png"
    
    plt.figure(figsize=(15, 5))

    plt.subplot(1, 3, 1)
    plt.imshow(original_img)
    plt.title("Original Image", fontsize=14)
    plt.axis('off')

    plt.subplot(1, 3, 2)
    plt.imshow(cam, cmap='jet')
    plt.title("Grad-CAM Heatmap", fontsize=14)
    plt.colorbar(fraction=0.046, pad=0.04)
    plt.axis('off')

    plt.subplot(1, 3, 3)
    plt.imshow(overlay)
    plt.title("Overlay", fontsize=14)
    plt.axis('off')

    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"âœ… Grad-CAM ç»“æœå·²ä¿å­˜è‡³: {output_path}")

    # === ç”Ÿæˆ AI æŠ¥å‘Š ===
    print("\nğŸ“ ç”ŸæˆAIæŠ¥å‘Š...")
    report = engine.generate(image_path)
    print(f"\nAIæŠ¥å‘Š:\n{report}")
    
    # === æ˜¾ç¤ºç–¾ç—…æ£€æµ‹ ===
    print("\nğŸ”¬ ç–¾ç—…æ£€æµ‹åˆ†æ...")
    with torch.no_grad():
        feature_map, global_features = model.encoder(input_tensor)
        disease_logits = model.disease_classifier(global_features)
        disease_probs = torch.sigmoid(disease_logits)[0].cpu().numpy()
    
    from inference_engine.model_definition import DISEASE_NAMES
    
    print("\næ£€æµ‹åˆ°çš„å¼‚å¸¸ï¼ˆæ¦‚ç‡>0.3ï¼‰ï¼š")
    detected = False
    for i, (name, prob) in enumerate(zip(DISEASE_NAMES, disease_probs)):
        if prob > 0.3:
            print(f"  - {name}: {prob:.2%}")
            detected = True
    
    if not detected:
        print("  æœªæ£€æµ‹åˆ°æ˜æ˜¾å¼‚å¸¸")
    
    print(f"\nğŸ‰ å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ° {output_path}")


if __name__ == '__main__':
    main()
